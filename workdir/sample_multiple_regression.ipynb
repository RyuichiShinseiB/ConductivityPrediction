{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading sample dataset (California housing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing()\n",
    "print(dataset.DESCR)\n",
    "df_exp = pl.DataFrame(dataset.data, dataset.feature_names)\n",
    "df_obj = pl.DataFrame(dataset.target, [\"HousingPrices\"])\n",
    "\n",
    "df_total = pl.concat([df_exp, df_obj], how=\"horizontal\")\n",
    "\n",
    "# Explanatory variable\n",
    "col_exp_vars = df_exp.columns\n",
    "# Objective variable\n",
    "col_obj_vars = df_obj.columns\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(\n",
    "    df_total.to_pandas().corr(), annot=True, cmap=\"cividis\", fmt=\".2f\", linewidths=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the following data preprocessing is performed.\n",
    "#### Step 1. Exclusion of outliers\n",
    "An outlier is data that differs in trend from other data.\n",
    "When outliers are included in the training data, the model tries to read their trends as well, which reduces generalization performance.\n",
    "\n",
    "Therefore, they should be removed.\n",
    "\n",
    "#### Step 2. Convert DataFrame to array\n",
    "This is because label names are not necessary during training and it is sufficient to know which column corresponds to which column.\n",
    "\n",
    "#### Step 3. Separate into training data and test data\n",
    "Train on training data and test generalization performance on test data.\n",
    "\n",
    "#### Step 4. Perform standardization\n",
    "Standardization is to convert the mean to 0 and the variance to 1 within an j.\n",
    "Specifically, the following equation is used for the conversion.\n",
    "$$x_{j_{std}}[i] = \\frac{x_{j}[i] - \\mu_{j}}{\\sqrt{\\sigma^2{_{j}}}}$$\n",
    "where,\n",
    "- $j$ is the item name, \n",
    "- $i$ is the index in the item.\n",
    "- $x_{j}[i]$ is the $i$-th data of item $j$\n",
    "- $\\mu_{j}=\\Sigma_{i=1}^{N} x_{j}[i] / N$ is the mean in term $j$.\n",
    "- $\\sigma^2{_{j}}=\\Sigma_{i=1}^{N} \\left\\{ x_{j}[i] - \\mu_{j} \\right\\}^2 / N$ is the m variance in item $j$.\n",
    "\n",
    "The advantage of standardization is that it allows for comparison by matching item-by-item scales.\n",
    "<details>\n",
    "<summary> Proofs for mean value and variance after standardization </summary>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_{j_{std}} &= \\frac{1}{N} \\Sigma_{i=1}^{N} x_{j_{std}}[i]\\\\\n",
    "&= \\frac{1}{N} \\Sigma_{i=1}^{N} \\frac{x_{j}[i] - \\mu_{j}}{\\sqrt{\\sigma^2{_{j}}}}\\\\\n",
    "&= \\frac{1}{\\sqrt{\\sigma^2{_{j}}}} \\left\\{ \n",
    "    \\frac{\\Sigma_{i=1}^{N} x_{j}[i]}{N} - \n",
    "    \\frac{\\Sigma_{i=1}^{N} \\mu_{j}}{N}\n",
    "\\right\\}\\\\\n",
    "&= \\frac{1}{\\sqrt{\\sigma^2{_{j}}}} \\left(\n",
    "    \\mu_{j} - \\mu_{j}\n",
    "\\right)\\\\\n",
    "&= 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\sigma^2}_{j_{std}} &= \\frac{1}{N } \\Sigma_{i=1}^{N} \\left\\{ x_{j_{std}}[i] - \\mu_{j_{std}} \\right\\}^2\\\\\n",
    "&= \\frac{1}{N } \\Sigma_{i=1}^{N} \\left\\{ x_{j_{std}}[i] - 0  \\right\\}^2\\\\\n",
    "&= \\frac{1}{N } \\Sigma_{i=1}^{N} \\left\\{ \\frac{x_{j}[i] - \\mu_{j}}{\\sqrt{\\sigma^2{_{j}}}} \\right\\}^2\\\\\n",
    "&= \\frac{1}{\\sigma^2{_{j}}} \\frac{1}{N } \\Sigma_{i=1}^{N} \\left\\{ x_{j}[i] - \\mu_{j} \\right\\}^2\\\\\n",
    "&= \\frac{1}{\\sigma^2{_{j}}} \\cdot \\sigma^2{_{j}}\\\\\n",
    "&= 1\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Exclusion of outliers\n",
    "quantiles = df_exp.quantile(0.95)\n",
    "\n",
    "df_prsd = df_total.filter(\n",
    "    [pl.col(col) < quantiles.select(col) for col in col_exp_vars]\n",
    ")\n",
    "df_prsd\n",
    "\n",
    "# Step 2: Convert DaraFrame to array\n",
    "X = df_prsd.select(pl.exclude(col_obj_vars)).to_numpy()\n",
    "y = df_prsd.select(col_obj_vars).to_numpy()\n",
    "\n",
    "# Step 3: Separate into training data and test data\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Perform standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "train_x_scaled = scaler.transform(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison before and after standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_col = 0\n",
    "histplot_df = pl.DataFrame({\n",
    "    \"Unscaled\": train_x[:, item_col].flatten(),\n",
    "    \"Scaled\": train_x_scaled[:, item_col].flatten(),\n",
    "    })\n",
    "cols = histplot_df.columns\n",
    "item_means = histplot_df.mean().to_numpy()\n",
    "mean_unscale, mean_scale = item_means[:, 0], item_means[:, 1]\n",
    "item_stds = histplot_df.std().to_numpy()\n",
    "std_unscale, std_scale = item_stds[:, 0], item_stds[:, 1]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "xrange = np.arange(-3, 8, 0.2)\n",
    "ax = sns.histplot(histplot_df.to_pandas(), bins=xrange, legend=True, ax=ax)\n",
    "\n",
    "ax.vlines(mean_unscale, 0, 900, colors=\"tab:blue\")\n",
    "ax.text(mean_unscale, 900, \"Mean of Unscaled\", horizontalalignment=\"center\")\n",
    "ax.vlines(mean_scale, 0, 900, colors=\"tab:orange\", label=\"Mean of scaled\")\n",
    "ax.text(mean_scale, 900, \"Mean of Scaled\", horizontalalignment=\"center\")\n",
    "\n",
    "ax.set_ylabel(f\"Count of {col_exp_vars[item_col]}\")\n",
    "ax.set_title(\"Comparison before and after standardization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition, training, and value prediction\n",
    "The following equation is a model for multiple linear regression.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{pred} &= w_0 + w_1 \\cdot x_1 + \\cdots + w_D \\cdot x_D\\\\\n",
    "&= w_0 + \\Sigma_{j=1}^{D} w_j \\cdot x_j\\\\\n",
    "&= \\boldsymbol{w}^T \\boldsymbol{x}\n",
    "\\end{aligned}\\\\\n",
    "\\boldsymbol{w} = \\left[ w_0 \\ w_1 \\ w_2 \\ \\cdots w_D \\right]^T, \n",
    "\\boldsymbol{x} = \\left[ 1 \\ x_1 \\ x_2 \\cdots x_D \\right]^T\n",
    "$$\n",
    "where,\n",
    "- $x_{j}$ is the value of the $j$ th item.\n",
    "- $w_j$ is the weight parameter (partial regression coefficient) of each item that is updated by training.\n",
    "- $D$ is the number of items.\n",
    "\n",
    "Mean squared error (MSE) and gradient descent are used to update the parameters.\n",
    "The MSE represents the error between the true value and the predicted value, and the parameters are updated using the gradient descent method to reduce the error.\n",
    "\n",
    "MSE\n",
    "$$\n",
    "E = \\frac{1}{N}\\Sigma_{i=1}^{N} \\left( \n",
    "    y_{pred_i} - y_{ture_i}\n",
    "\\right) ^ 2 \\\\\n",
    "$$\n",
    "where, $N$ is the number of data.\n",
    "\n",
    "Gradient Descent\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\nabla_{\\boldsymbol{w}}E\\\\\n",
    "$$\n",
    "where, $\\eta$ is learning rate.\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"./figures/gradient_descent.svg\" alt=\"The idea of the gradient descent\" title=\"The idea of the gradient descenthe\">\n",
    "        <figcaption>\n",
    "            The gradient descent method is based on the fact that the gradient of a continuous function represents the direction of the minima.\n",
    "        </figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition\n",
    "model = LinearRegression()\n",
    "# training\n",
    "model.fit(train_x_scaled, train_y)\n",
    "# Value prediction\n",
    "y_pred = model.predict(train_x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Partial Regression Coefficients\n",
    "The following values is the partial regression coefficients of each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xi, wi in zip(col_exp_vars, model.coef_[0]):\n",
    "    print(\"{0:7}: {1:6.3f}\".format(xi, wi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization using bar charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.bar(\n",
    "    col_exp_vars,\n",
    "    model.coef_[0],\n",
    "    edgecolor=\"black\",\n",
    "    facecolor=\"None\",\n",
    "    hatch=\".....\"\n",
    ")\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.set_ylabel(\"Partial regression coefficient\")\n",
    "ax.set_xticks(range(len(col_exp_vars)), col_exp_vars, rotation=45)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.grid(which=\"both\", axis=\"y\")\n",
    "ax.tick_params(axis=\"y\", direction=\"in\")\n",
    "# ax.tick_params(axis=\"x\", direction=\"in\")\n",
    "\n",
    "ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate of the model\n",
    "Refer to coefficient of determination ($R^2$) and MSE\n",
    "\n",
    "If $R^2$ exceeds 0.5, it can be said that some trend has been obtained.\n",
    "\n",
    "If the difference between the test data and the training data is large for MSE, it means that overfitting has occured and generalization performance is low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = model.score(train_x_scaled, train_y)\n",
    "mse_train = mean_squared_error(train_y, y_pred)\n",
    "y_pred_test = model.predict(scaler.transform(test_x))\n",
    "mse_test = mean_squared_error(test_y, y_pred_test)\n",
    "print(\"Coefficient of determination: \", r2)\n",
    "print(f\"Mean squared error of training data: {mse_train: 0.4f}\")\n",
    "print(f\"Mean squared error of test data: {mse_test: 0.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
